\section{Related Work}
\label{sec:formatting}

We have summarized research work on different aspects of UVDT.

\textbf{Vehicle Detection.}
UVDT mainly uses LiDAR and cameras for vehicle object detection.
Several recent studies have explored the fusion of LiDAR and camera data for vehicle detection,demonstrating significant improvements in both accuracy and robustness. 
For instance, MVDNet (2018) utilizes a multi-view fusion approach, where LiDAR point clouds are projected to different views (such as bird’s-eye view and front view) and matched with camera images, combining depth information from LiDAR and texture information from the camera. 
This method improves detection performance, especially in complex environments. 
Building on this, PointPillars (2019) introduces a fast encoder for LiDAR data, using a grid-based encoding approach that could potentially be integrated with camera data in future fusion methods to enhance detection speed and efficiency.
Another study (2020) combines LiDAR and camera data by feeding them into separate convolutional neural networks, where the features extracted from both modalities are fused to perform object detection. 
This approach has shown significant improvements in detection performance, particularly in dynamic environments with occlusions or sparse LiDAR data.
Lastly, ST-MVDNet (2021) introduces a self-training framework using a teacher-student mutual learning mechanism, where the teacher network is trained on the fused LiDAR and camera data, while the student network is exposed to strong data augmentation simulating missing sensor modalities. 

This approach enhances the model’s robustness against sensor failures by ensuring consistency between the teacher and student models, allowing the system to better handle missing or noisy data during inference.
These methods collectively highlight the importance of sensor fusion and advanced learning techniques in achieving robust and accurate vehicle detection, even in challenging conditions.
We use the PointPillars deep learning method which has shown good performance.

\textbf{Multi camera multi-target tracking.}
Most existing algorithms, with a few exceptions, can be seen as special cases of the multi-modal fusion problem. 
These methods organize the input data using a graph structure, where edges represent relationships between modalities, and nodes represent different targets or states. 
Algorithms that can be solved in polynomial time typically handle specific modalities or time-continuous edges, with some also utilizing maximum flow or matching algorithms. 
Methods that leverage global information (beyond just time continuity or modality constraints) can significantly improve performance, but they are usually NP-hard due to the involvement of combinatorial optimization. 
In some cases, marginal terms or local constraints are added to ensure completeness. 
To enhance model expressiveness, some studies have employed higher-order relations, although the gains diminish significantly as complexity increases. 
Joint optimization and iterative optimization strategies have also been widely used to improve performance.
We use Joint Integrated Probabilistic Data Association (JIPDA), which combines data from multiple sensors and optimizes probabilistic associations to effectively handle data uncertainty and missing information in target tracking, improving tracking accuracy and robustness in complex environments.


\textbf{Vehicle re identification.}
The appearance of vehicles can be described through color, texture, and shape features. 
While color and texture features are commonly used for identifying the vehicle's appearance, shape features provide important information about the vehicle's structure. 
The impact of lighting changes is typically adjusted through color normalization, sample-based techniques, or luminance transfer functions, which can be optimized via supervised or unsupervised learning. 
To enhance distinguishability, salient information about the vehicle or features related to specific body parts are often leveraged. 
These features can be extracted directly from images or mapped onto 3D vehicle models for improved identification accuracy. 
Shape features, particularly the contours and structure of the vehicle body, can effectively differentiate between different types of vehicles and provide additional distinguishing information, especially in cases of significant viewpoint variation.

The most advanced technologies in the field of vehicle re-identification currently include the combination of deep convolutional neural networks (CNN) for feature extraction and metric learning, particularly with the integration of cross-view and cross-domain learning techniques, the use of Generative Adversarial Networks (GAN) for image enhancement, and the fusion of multi-sensor data (such as cameras, radar, and LiDAR) to improve the model's robustness and accuracy in complex environments.

We designed a ResNet-50 network for vehicle re-identification by drawing inspiration from Re-ID.
The network is capable of performing this task effectively.

\textbf{twin.}
After obtaining all the data, we will proceed with vehicle control. 
Kaleb Ben Naveed et al. proposed a hierarchical reinforcement learning-based method for autonomous driving trajectory planning and control. 
By training a reinforcement learning agent in a simulation environment, this method optimizes the vehicle's control strategy according to environmental changes, achieving efficient trajectory planning and dynamic path adjustment.\cite{Alpher22}
R. Barea et al. proposed a deep reinforcement learning (DRL)-based control method, where an agent is trained in the CARLA simulation platform to autonomously learn and optimize the vehicle's control strategies (such as throttle, brake, and steering) to ensure safe and smooth driving.\cite{Alpher21}
We use a trajectory smoothing algorithm to control the vehicle's trajectory, enabling the vehicle to more accurately follow the planned path, especially in situations with many road curves or irregular road surfaces. 
Compared to precise real-time optimization control, the trajectory smoothing algorithm typically involves fewer real-time computations, reducing reliance on computational resources while maintaining effectiveness and improving real-time performance.
