\section{Experiments}

We conducted the following experiments: 
(a) 3D object detection using PointPillars deep learning for LiDAR, 
(b) performing multi-target tracking under single-intersection and multi-intersection scenarios, 
(c) vehicle re-identification between intersections using a ReID network, and 
(d) implementing trajectory digital twins in Carla.

\subsection{preparation}

\textbf{Simulation Environment.}
We conducted simulation experiments in Carla, whose advantage lies in its provision of a highly realistic virtual environment capable of accurately simulating complex traffic scenarios and diverse driving conditions. 
Carla supports flexible sensor configurations, such as cameras and LiDAR, facilitating multimodal data acquisition and fusion experiments\cite{Alpher22e}. 
We selected Town10 and Town1 in Carla as our simulation scenarios, which offer the benefit of providing highly realistic and diverse virtual environments, enabling precise simulation and replication of real traffic scenarios. 
The complex urban structure and dense traffic flow of Town10 can simulate highly dynamic real traffic environments, while the simple layout and clear rules of Town1 make it easy to construct controlled experimental scenarios.

\textbf{Data Collection.}
In CARLA's Town10 scenario, a single intersection location is selected, with a LiDAR placed at the center and six cameras arranged around it to achieve 360-degree omnidirectional perception coverage. 
This setup allows for the acquisition of rich point cloud data and image information, making it suitable for target recognition and tracking in complex traffic environments. 
By configuring the LiDAR with 64 channels, a detection range of 100 meters, 250,000 points per second, and a rotation frequency of 20 Hz, the radar can efficiently generate high-density and accurate point cloud data. 
Additionally, one camera is placed at the front and rear of the vehicle, and one camera is positioned on each of the four roads to the left and right. 
The cameras have a resolution of 1920x1080 pixels and a field of view of 90 degrees, ensuring the capture of wide-angle image data in high definition. 
The collected data includes point cloud data from the LiDAR and image data from the cameras. 
The radar data is saved as MAT files with timestamps, while the camera data is saved as images for each frame in six directions. 
The data structure follows the storage format of the Panda dataset, making the data more standardized for subsequent processing and fusion\cite{Alpher21c}. 
Each frame of radar data stores information such as point cloud objects, timestamps, positions relative to the ego vehicle, and detected 3D bounding boxes. 
Each frame of camera data stores image data from six directions, positions relative to the ego vehicle, timestamps, and detected 2D bounding boxes.

\subsection{Experimental effect}

\begin{table*}[t] 
	\centering
	\caption{Camera evaluation metric results}
	\label{tab:example}
	\begin{tabular}{ccccccccccc}
		\toprule
		Camera & Rcll & Prcn & FTR & FP & FN & IDS & MT & ML & MOTA & MOTP\\
		\midrule
		1 & 25.2599 & 75.3488 & 0.318 & 159 & 1438 & 9 & 25 & 66.6667 & 16.5281 & 84.5750\\
		2 & 23.8684 & 56.6213 & 1.212 & 606 & 2523 & 11 & 13.3333 & 46.6667 & 5.2505 & 82.7815\\
		\bottomrule
	\end{tabular}
\end{table*}

\subsection{Details Explanation}


\subsection{Limitations and Future Directions}

\textbf{Problems Encountered in The Experiment.}
During our experiments, we encountered the following issues:  
1.When matching trajectories across multiple intersections, images of vehicles corresponding to each trajectory are needed to extract re-identification appearance features. 
Acquiring these images is difficult, and the methods used can affect accuracy.  
2.When matching trajectories, it is necessary to associate trajectories from all intersections. 
However, due to ID switching, the trajectory of the same vehicle at a single intersection may be fragmented, making integration complex. 
If only one trajectory is selected as the current intersection's trajectory for a vehicle, issues arise when the same vehicle returns to the intersection.

\textbf{Shortcomings of Current Research.}
Although our experiments have achieved some success, there are still some limitations. 
For example, the detection accuracy of PointPillars is not high, resulting in suboptimal tracking performance. 
Additionally, false detections may occur when acquiring images of vehicles corresponding to current trajectories, potentially leading to trajectory matching errors.

\textbf{Vision of The Future.}
Our experiments were conducted under offline conditions, meaning they lacked real-time capabilities. 
In future work, we aim to transition these experiments to an online framework to achieve high real-time performance, thereby enhancing their experimental value.